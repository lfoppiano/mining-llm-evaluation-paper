@article{kokon2023chatgpt,
	doi = {10.1016/j.inffus.2023.101861},
  
	url = {https://doi.org/10.1016%2Fj.inffus.2023.101861},
  
	year = 2023,
	month = {nov},
	publisher = {Elsevier {BV}},
  
	volume = {99},
  
	pages = {101861},
  
	author = {Jan Koco{\'{n}} and Igor Cichecki and Oliwier Kaszyca and Mateusz Kochanek and Dominika Szyd{\l}o and Joanna Baran and Julita Bielaniewicz and Marcin Gruza and Arkadiusz Janz and Kamil Kanclerz and Anna Koco{\'{n}} and Bart{\l}omiej Koptyra and Wiktoria Mieleszczenko-Kowszewicz and Piotr Mi{\l}kowski and Marcin Oleksy and Maciej Piasecki and {\L}ukasz Radli{\'{n}}ski and Konrad Wojtasik and Stanis{\l}aw Wo{\'{z}}niak and Przemys{\l}aw Kazienko},
  
	title = {{ChatGPT}: Jack of all trades, master of none},
  
	journal = {Information Fusion}
}

@misc{moradi2022gpt3,
      title={GPT-3 Models are Poor Few-Shot Learners in the Biomedical Domain}, 
      author={Milad Moradi and Kathrin Blagec and Florian Haberl and Matthias Samwald},
      year={2022},
      eprint={2109.02555},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{hoffmann2022training,
  author = {Hoffmann, J. and Borgeaud, S. and Arthur, M. and Buchatskaya, E. and Cai, T. Y. and Eliza, R. and Sifre, L.},
  title = {Training Compute-optimal Large Language Models},
  year = {2022},
  doi = {10.48550/arxiv.2203.15556}
}


@article{risch2021semantic,
  author = {Risch, J. and Möller, T. and Gutsch, J. and Pietsch, M.},
  title = {Semantic answer similarity for evaluating question answering models},
  journal = {Proceedings of the 3rd Workshop on Machine Reading for Question Answering},
  year = {2021},
  doi = {10.18653/v1/2021.mrqa-1.15}
}

@inproceedings{harper2021semeval,
    title = "{S}em{E}val-2021 Task 8: {M}eas{E}val {--} Extracting Counts and Measurements and their Related Contexts",
    author = "Harper, Corey  and
      Cox, Jessica  and
      Kohler, Curt  and
      Scerri, Antony  and
      Daniel Jr., Ron  and
      Groth, Paul",
    booktitle = "Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.semeval-1.38",
    doi = "10.18653/v1/2021.semeval-1.38",
    pages = "306--316",
    abstract = "We describe MeasEval, a SemEval task of extracting counts, measurements, and related context from scientific documents, which is of significant importance to the creation of Knowledge Graphs that distill information from the scientific literature. This is a new task in 2021, for which over 75 submissions from 25 participants were received. We expect the data developed for this task and the findings reported to be valuable to the scientific knowledge extraction, metrology, and automated knowledge base construction communities.",
}


@article{tang2023struc,
  title={Struc-Bench: Are Large Language Models Really Good at Generating Complex Structured Data?},
  author={Tang, Xiangru and Zong, Yiming and Zhao, Yilun and Cohan, Arman and Gerstein, Mark},
  journal={arXiv preprint arXiv:2309.08963},
  year={2023}
}


@article{gonzalez2023yes,
  title={Yes but.. Can ChatGPT identify entities in historical documents?},
  author={Gonz{\'a}lez-Gallardo, Carlos-Emiliano and Boros, Emanuela and Girdhar, Nancy and Hamdi, Ahmed and Moreno, Jose G and Doucet, Antoine},
  journal={arXiv preprint arXiv:2303.17322},
  year={2023}
}

@article{ma2023large,
  author = {Ma, Y. and Cao, Y. and Hong, Y. and Sun, A.},
  title = {Large language model is not a good few-shot information extractor, but a good reranker for hard samples!},
  year = {2023},
  doi = {10.48550/arxiv.2303.08559}
}

@inproceedings{lin2023llmeval,
    title = "{LLM}-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models",
    author = "Lin, Yen-Ting  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 5th Workshop on NLP for Conversational AI (NLP4ConvAI 2023)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.nlp4convai-1.5",
    doi = "10.18653/v1/2023.nlp4convai-1.5",
    pages = "47--58",
    abstract = "We propose LLM-Eval, a unified multi-dimensional automatic evaluation method for open-domain conversations with large language models (LLMs). Existing evaluation methods often rely on human annotations, ground-truth responses, or multiple LLM prompts, which can be expensive and time-consuming. To address these issues, we design a single prompt-based evaluation method that leverages a unified evaluation schema to cover multiple dimensions of conversation quality in a single model call. We extensively evaluate the performance of LLM-Eval on various benchmark datasets, demonstrating its effectiveness, efficiency, and adaptability compared to state-of-the-art evaluation methods. Our analysis also highlights the importance of choosing suitable LLMs and decoding strategies for accurate evaluation results. LLM-Eval offers a versatile and robust solution for evaluating open-domain conversation systems, streamlining the evaluation process and providing consistent performance across diverse scenarios.",
}
@article{reimers2019sentencebert,
  author = {Reimers, N. and Gurevych, I.},
  title = {sentence-bert: sentence embeddings using siamese bert-networks},
  year = {2019},
  doi = {10.18653/v1/d19-1410}
}

@inproceedings{beltagy2020scibert,
	title        = {{S}ci{BERT}: A Pretrained Language Model for Scientific Text},
	author       = {Beltagy, Iz  and Lo, Kyle  and Cohan, Arman},
	year         = 2019,
	month        = nov,
	booktitle    = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
	publisher    = {Association for Computational Linguistics},
	address      = {Hong Kong, China},
	pages        = {3615--3620},
	doi          = {10.18653/v1/D19-1371},
	url          = {https://aclanthology.org/D19-1371},
	abstract     = {Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SciBERT, a pretrained language model based on BERT (Devlin et. al., 2018) to address the lack of high-quality, large-scale labeled scientific data. SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/.}
}

@article{yasunaga2020linkbert,
  author = {Yasunaga, M. and Leskovec, J. and Liang, P.},
  title = {Linkbert: pretraining language models with document links},
  year = {2022},
  doi = {10.48550/arxiv.2203.15827}
}

@misc{jin2023large,
      title={Can Large Language Models Infer Causation from Correlation?}, 
      author={Zhijing Jin and Jiarui Liu and Zhiheng Lyu and Spencer Poff and Mrinmaya Sachan and Rada Mihalcea and Mona Diab and Bernhard Schölkopf},
      year={2023},
      eprint={2306.05836},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{lfoppiano2021supermat,
  author = {Foppiano, L. and Dieb, T. and Suzuki, A. and Castro, P. and Iwasaki, S. and Uzuki, A. and Echevarria, M. and Meng, Y. and Terashima, K. and Romary, L. and Takano, Y. and Ishii, M.},
  title = {Supermat: construction of a linked annotated dataset from superconductors-related publications},
  journal = {Science and Technology of Advanced Materials Methods},
  year = {2021},
  volume = {1},
  issue = {1},
  pages = {34-44},
  doi = {10.1080/27660400.2021.1918396}
}

@article{lfoppiano2023automatic,
  author = {Foppiano, L. and Castro, P. and Suarez, P. and Terashima, K. and Takano, Y. and Ishii, M.},
  title = {Automatic extraction of materials and properties from superconductors scientific literature},
  journal = {Science and Technology of Advanced Materials Methods},
  year = {2023},
  volume = {3},
  issue = {1},
  doi = {10.1080/27660400.2022.2153633}
}

@misc{min2023factscore,
      title={FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation}, 
      author={Sewon Min and Kalpesh Krishna and Xinxi Lyu and Mike Lewis and Wen-tau Yih and Pang Wei Koh and Mohit Iyyer and Luke Zettlemoyer and Hannaneh Hajishirzi},
      year={2023},
      eprint={2305.14251},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{harper2021semeval2021,
    title = "{S}em{E}val-2021 Task 8: {M}eas{E}val {--} Extracting Counts and Measurements and their Related Contexts",
    author = "Harper, Corey  and
      Cox, Jessica  and
      Kohler, Curt  and
      Scerri, Antony  and
      Daniel Jr., Ron  and
      Groth, Paul",
    editor = "Palmer, Alexis  and
      Schneider, Nathan  and
      Schluter, Natalie  and
      Emerson, Guy  and
      Herbelot, Aurelie  and
      Zhu, Xiaodan",
    booktitle = "Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.semeval-1.38",
    doi = "10.18653/v1/2021.semeval-1.38",
    pages = "306--316",
    abstract = "We describe MeasEval, a SemEval task of extracting counts, measurements, and related context from scientific documents, which is of significant importance to the creation of Knowledge Graphs that distill information from the scientific literature. This is a new task in 2021, for which over 75 submissions from 25 participants were received. We expect the data developed for this task and the findings reported to be valuable to the scientific knowledge extraction, metrology, and automated knowledge base construction communities.",
}


@article{hatakeyama2023prompt,
author = {Kan Hatakeyama-Sato and Naoki Yamane and Yasuhiko Igarashi and Yuta Nabae and Teruaki Hayakawa},
title = {Prompt engineering of GPT-4 for chemical research: what can/cannot be done?},
journal = {Science and Technology of Advanced Materials: Methods},
volume = {3},
number = {1},
pages = {2260300},
year = {2023},
publisher = {Taylor & Francis},
doi = {10.1080/27660400.2023.2260300},


URL = { 
    
        https://doi.org/10.1080/27660400.2023.2260300
    
    

},
eprint = { 
    
        https://doi.org/10.1080/27660400.2023.2260300
}

}




@article{taylor2022galactica,
  title={Galactica: A large language model for science},
  author={Taylor, Ross and Kardas, Marcin and Cucurull, Guillem and Scialom, Thomas and Hartshorn, Anthony and Saravia, Elvis and Poulton, Andrew and Kerkez, Viktor and Stojnic, Robert},
  journal={arXiv preprint arXiv:2211.09085},
  year={2022}
}
@inproceedings{foppiano2019quantities,
	title        = {Automatic Identification and Normalisation of Physical Measurements in Scientific Literature},
	author       = {Foppiano, Luca and Romary, Laurent and Ishii, Masashi and Tanifuji, Mikiko},
	year         = 2019,
	booktitle    = {Proceedings of the ACM Symposium on Document Engineering 2019},
	location     = {Berlin, Germany},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {DocEng '19},
	doi          = {10.1145/3342558.3345411},
	isbn         = 9781450368872,
	url          = {https://doi.org/10.1145/3342558.3345411},
	abstract     = {We present Grobid-quantities, an open-source application for extracting and normalising measurements from scientific and patent literature. Tools of this kind, aiming to understand and make unstructured information accessible, represent the building blocks for large-scale Text and Data Mining (TDM) systems. Grobid-quantities is a module built on top of Grobid [6] [13], a machine learning framework for parsing and structuring PDF documents. Designed to process large quantities of data, it provides a robust implementation accessible in batch mode or via a REST API. The machine learning engine architecture follows the cascade approach, where each model is specialised in the resolution of a specific task. The models are trained using CRF (Conditional Random Field) algorithm [12] for extracting quantities (atomic values, intervals and lists), units (such as length, weight) and different value representations (numeric, alphabetic or scientific notation). Identified measurements are normalised according to the International System of Units (SI). Thanks to its stable recall and reliable precision, Grobid-quantities has been integrated as the measurement-extraction engine in various TDM projects, such as Marve (Measurement Context Extraction from Text), for extracting semantic measurements and meaning in Earth Science [10]. At the National Institute for Materials Science in Japan (NIMS), it is used in an ongoing project to discover new superconducting materials. Normalised materials characteristics (such as critical temperature, pressure) extracted from scientific literature are a key resource for materials informatics (MI) [9].},
	articleno    = 24,
	numpages     = 4,
	keywords     = {Physical quantities, Measurements, Units of measurements, TDM, Machine Learning}
}